{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dcbb7ba-300e-46ce-ad84-cc410b01a378",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nd2\n",
    "\n",
    "from ops.imports import *\n",
    "from ops.process import Align\n",
    "import ops.firesnake\n",
    "from ops.firesnake import Snake\n",
    "\n",
    "import ops.in_situ\n",
    "import tifffile\n",
    "import numpy as np\n",
    "from natsort import natsorted\n",
    "\n",
    "from matplotlib import pyplot as plt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "142a06bc-ff05-4471-9d0b-4c114851ecc5",
   "metadata": {},
   "source": [
    "# steps to run the analysis \n",
    "1) make sure to get the offset from **PhenotypeAlignment** notebook\n",
    "2) Use the **folderPath10** as the directory to read the ISS cycle 0 files to use for alignment\n",
    "3) Use the **folderPath20** as the nd2 files for phenotype (nfish) for this\n",
    "4) make sure you choose the correct string identifer to look for for the filename like **'WellA1'**\n",
    "5) **directory_df_10x_iter** = is the place where the analyzed barcode are kept after the ISS analysis\n",
    "6) **'get_matched_df_from_phenotype'** this function is called with the offset\n",
    "7) **nfish_saved_folder** = use this one where the analyzed nfish are kept (the csv files)\n",
    "8) **save_matched_df**(input_20x, df_merged_with_nf, 'nfish_aligned_a1_reanalyzed_A1_3_sgrna') - use this one to save the anlyzed images\n",
    "9) use **postProcessingPhenotype** to analyze the saved images\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72ef04f2-d92a-4bea-861c-869b9b23f4e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEBUG_PRINT = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c0d0c2f-4d1f-42b6-97c6-37d7eefdf45a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "micronPpixel10x2 = 0.643259084266003\n",
    "micronPpixel20x2 = 0.1625\n",
    "height10x = 2304*micronPpixel10x2\n",
    "height20x = 2304*micronPpixel20x2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a06809d2-1527-4fd9-bde9-850980867486",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEBUG_PRINT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efcc1ab7-9dee-4e47-8e07-25c6c4243bfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#folderPath10 = '/vf/users/HiTIF/data/sagarm2/OPSData/NovData14sgRNA/ISSData/A1/20241115_114303_966_IQL_POP_043_ISS01/'\n",
    "#folderPath20 = '//vf/users/HiTIF/data/sagarm2/OPSData/NovData14sgRNA/NFISH/20241114_120846_493_IQL_POP_043_NFISH/'\n",
    "\n",
    "# dec 600 sgrna \n",
    "#folderPath10 = '/vf/users/HiTIF/data/sagarm2/OPSData/DecData600sgRNA/ISSData/20241205_132433_653_IQL_POP_044_ALTminiLib_LowDox_2days_ISS01/'\n",
    "#folderPath20 = '/vf/users/HiTIF/data/sagarm2/OPSData/DecData600sgRNA/20241204_125133_089_IQL_POP_044_ALTminiLib_LowDox_2days_NFISH/'\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "WILDCARDS = dict(well='A1', tile='0')\n",
    "\n",
    "#\\ISSDataSet\\B1\\ISS01\n",
    "\n",
    "searchRPE10=folderPath10+\"/*.nd2\"\n",
    "input_files10 = natsorted(glob(searchRPE10))\n",
    "\n",
    "\n",
    "searchRPE20=folderPath20+\"/*.nd2\"\n",
    "input_files20_all = natsorted(glob(searchRPE20))\n",
    "input_files20_list = [file_name for file_name in input_files20_all if 'WellA2' in file_name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df53b13f-3e05-4445-b606-54e4de45cf15",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#folderPath10 = \"/vf/users/HiTIF/data/sagarm2/OPSData/NovData14sgRNA/ISSData/B1/20241115_122644_917_IQL_POP_043_ISS01_B1/\"\n",
    "#folderPath20 = \"/vf/users/HiTIF/data/sagarm2/OPSData/NovData14sgRNA/NFISH/20241114_120846_493_IQL_POP_043_NFISH/\"\n",
    "\n",
    "\n",
    "#folderPath10 = \"/vf/users/HiTIF/data/sagarm2/OPSData/jan2025NFISHISS_all/ISSData/A2/20250123_145415_171_IQL_POP_046_ALTminiLib_3_ISS01/\"\n",
    "folderPath10 = \"/vf/users/HiTIF/data/sagarm2/OPSData/jan2025NFISHISS_all/ISSData/A2/20250123_145415_171_IQL_POP_046_ALTminiLib_3_ISS01/\"\n",
    "folderPath20 = \"/vf/users/HiTIF/data/sagarm2/OPSData/jan2025NFISHISS_all/20250123_095504_119_IQL_POP_046_ALTminiLib_3_NFISH/A2/\"\n",
    "\n",
    "WILDCARDS = dict(well='A2', tile='0')\n",
    "#\\ISSDataSet\\B1\\ISS01\n",
    "\n",
    "searchRPE10=folderPath10+\"/*.nd2\"\n",
    "input_files10_all = natsorted(glob(searchRPE10))\n",
    "input_files10 = [file_name for file_name in input_files10_all if 'WellA2' in file_name]\n",
    "\n",
    "\n",
    "searchRPE20=folderPath20+\"/*.nd2\"\n",
    "input_files20_all = natsorted(glob(searchRPE20))\n",
    "input_files20_list = [file_name for file_name in input_files20_all if 'WellA2' in file_name]\n",
    "\n",
    "print(len(input_files10), len(input_files20_list))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6be56287-4430-4b9e-83ab-80402aa15fc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate all df10 locations\n",
    "# run once for single well\n",
    "\n",
    "searchRPE1=folderPath10+\"/*.nd2\"\n",
    "input_files1 = natsorted(glob(searchRPE1))\n",
    "input_files10 = [file_name for file_name in input_files10_all if 'WellA2' in file_name]\n",
    "\n",
    "_, micronPpixel10x2, _, _ = readPhenotypeImageToExtractLocation2(input_files1[0]) ## getting metadata infor\n",
    "\n",
    "\n",
    "height10x = 2304*micronPpixel10x2\n",
    "#height20x = 2304*micronPpixel20x2\n",
    "\n",
    "data10 = []\n",
    "\n",
    "for fileName in input_files10:\n",
    "    xpos, ypos = get_position_from_nd2(fileName)\n",
    "    x1_, x2_, y1_, y2_ = calculate_FOV_coord( xpos, ypos, height10x)\n",
    "    data10.append((fileName, x1_, x2_, y1_, y2_))\n",
    "\n",
    "df_loc10_all = pd.DataFrame(data10, columns=['filename','x1','x2', 'y1','y2'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8689cc76-a9ac-4915-86b4-cb0d744d8bd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_files20_list[150]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4724866f-db29-48f2-9c0c-f280f4b58f5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_loc10_all.iloc[0]['filename']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a852011-2a9e-4db8-b1d1-b192e0c94e43",
   "metadata": {},
   "outputs": [],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98929482-78bd-46f3-97be-75f97092adce",
   "metadata": {},
   "source": [
    "## offset backup\n",
    "nov14sgrna A1,  xoffset=-26, yoffset=-20\n",
    "\n",
    "Nov600sgrna, B1,  xoffset=-39, yoffset=-20\n",
    "\n",
    "\n",
    "dec mini screen, A1, xoffset=44, yoffset=38\n",
    "\n",
    "dec mini, A2, xoffset=40, yoffset=12\n",
    "\n",
    "jan 2025\n",
    "A1, xoffset=-185, yoffset=-254\n",
    "\n",
    "A2, xoffset=-186, yoffset=-235\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f0f7f60-08e9-4c26-a42b-eba78d57a1b4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#directory_df_10x_iter = '/gpfs/gsfs10/users/sagarm2/OPS_POP_2022_works/example_data/outputNFISHALLWELLS/B3/'\n",
    "directory_df_10x_iter = '/gpfs/gsfs10/users/sagarm2/OPS_POP_2022_works/example_data/miniscreen/jan2024/A2_lowT/'\n",
    "#directory_df_10x_iter = '/gpfs/gsfs10/users/sagarm2/OPS_POP_2022_works/example_data/nov14sgrna/B1/'\n",
    "\n",
    "DEBUG_PRINT = False\n",
    "#starttime = time.time()\n",
    "\n",
    "\n",
    "nfish_saved_folder = 'nfish_df_A2_thresh9'\n",
    "\n",
    "for k20 in range(0,len(input_files20_list)):\n",
    "    \n",
    "    input_20x = input_files20_list[k20]\n",
    "    #df_merged_with_nf = get_matched_df_from_phenotype(input_20x)\n",
    "    #df_merged_with_nf = get_matched_df_from_phenotype(input_20x, xoffset=3, yoffset=138 )\n",
    "    #df_merged_with_nf = get_matched_df_from_phenotype(input_files20_list[k20], directory_df_10x_iter, df_loc10_all, xoffset=3, yoffset=111.4)\n",
    "\n",
    "    try:\n",
    "        df_merged_with_nf = get_matched_df_from_phenotype(input_20x, directory_df_10x_iter,df_loc10_all, nfish_saved_folder,  xoffset=-185, yoffset=-254) \n",
    "    except KeyError as e:\n",
    "        print(f\"KeyError encountered: {e}\")\n",
    "        continue\n",
    "    except FileNotFoundError as e:\n",
    "        print(f\"FileNotFoundError encountered: {e}\")\n",
    "        continue\n",
    "    #df_merged_with_nf = get_matched_df_from_phenotype(input_files20_list[k20], directory_df_10x_iter, df_loc10_all, xoffset=-26, yoffset=-20)\n",
    "    if df_merged_with_nf.empty:\n",
    "        if DEBUG_PRINT: \n",
    "            print(k20, input_20x, 'empty')\n",
    "            \n",
    "    else:\n",
    "        #save_matched_df(input_20x, df_merged_with_nf)\n",
    "        save_matched_df(input_20x, df_merged_with_nf, 'aligned_a1_lowT') # make sure this folder exits\n",
    "\n",
    "\n",
    "#endTime  time.time() - starttime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3754995-e2fe-4675-8899-3563dd5b1360",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_20x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3441a643-a24d-42d5-8612-bcec7947cd3f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e67cea8a-a734-4ae8-a26c-bf1d4a971424",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_loc10_all.iloc[1]['filename']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6b36206-7c79-492f-8f44-6cbed8801a48",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import concurrent.futures\n",
    "import time\n",
    "\n",
    "StartTime = time.time()\n",
    "\n",
    "# make sure the nfish_df (the folder to save nfish data) is properly marked. This code reads from the nfish_df folder\n",
    "\n",
    "#directory_df_10x_iter = '/gpfs/gsfs10/users/sagarm2/OPS_POP_2022_works/example_data/nov14sgrna/A1_3/'\n",
    "directory_df_10x_iter = '/gpfs/gsfs10/users/sagarm2/OPS_POP_2022_works/example_data/miniscreen/jan2024/A2_lowT/'\n",
    "\n",
    "nfish_saved_folder = 'nfish_df_A2_thresh9'\n",
    "\n",
    "## try to minimize output-to-console as much as possible\n",
    "## causess process pool executor to run out of memory over time\n",
    "## use other methods fo\n",
    "DEBUG_PRINT = False \n",
    "\n",
    "\n",
    "def process_file(input_20x):\n",
    "    # Simulate your file processing\n",
    "    print('Processing file:', input_20x)\n",
    "    \n",
    "    # Get the data for the input file\n",
    "    #df_merged_with_nf = get_matched_df_from_phenotype(input_20x, directory_df_10x_iter, df_loc10_all, xoffset=3, yoffset=111.4)#B2\n",
    "    #df_merged_with_nf = get_matched_df_from_phenotype(input_20x, directory_df_10x_iter,df_loc10_all, xoffset=-30, yoffset=-16) #B1\n",
    "    #df_merged_with_nf = get_matched_df_from_phenotype(input_20x, directory_df_10x_iter,df_loc10_all, nfish_saved_folder, xoffset=40, yoffset=12 ) #B1\n",
    "    try:\n",
    "        df_merged_with_nf = get_matched_df_from_phenotype(input_20x, directory_df_10x_iter,df_loc10_all, nfish_saved_folder, xoffset=-186, yoffset=-235) #\n",
    "    except KeyError as e:\n",
    "        print(f\"KeyError encountered: {e}\")\n",
    "        #continue\n",
    "    except FileNotFoundError as e:\n",
    "        print(f\"FileNotFoundError encountered: {e}\")\n",
    "        #continue\n",
    "\n",
    "    # Check if the dataframe is empty\n",
    "    if df_merged_with_nf.empty:\n",
    "        if DEBUG_PRINT:\n",
    "            print('Empty dataframe for:', input_20x)\n",
    "    else:\n",
    "        # Save the matched dataframe\n",
    "        save_matched_df(input_20x, df_merged_with_nf, 'aligned_a2_lowT')  # Ensure the folder exists # this is the folder wher ealigned data is saved # same path as nfish_df\n",
    "\n",
    "# Set the maximum number of processes (for CPU-bound tasks, use processes instead of threads)\n",
    "max_processes = 8  # Adjust based on your system's CPU core count\n",
    "\n",
    "# Create a ProcessPoolExecutor for multi-processing\n",
    "with concurrent.futures.ProcessPoolExecutor(max_workers=max_processes) as executor:\n",
    "    # Submit tasks to the executor\n",
    "    futures = [executor.submit(process_file, input_20x) for input_20x in input_files20_list[1420:]]\n",
    "\n",
    "    # Wait for all processes to complete\n",
    "    concurrent.futures.wait(futures)\n",
    "\n",
    "# Calculate the total time taken\n",
    "timeTaken = time.time() - StartTime\n",
    "print(f'Time taken: {timeTaken} seconds')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98c40ad1-82a3-41c7-9df9-bce45f5882f3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "len(input_files20_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4c4f0c2-feee-4d5f-9633-0d88db5cf984",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "len(input_files20_list)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa23cc68-26bc-43d0-92bd-1e3b306c0950",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged_with_nf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3b443f9-ad25-4376-bb4f-4766a9065327",
   "metadata": {},
   "outputs": [],
   "source": [
    "directory_path = '/vf/users/HiTIF/data/sagarm2/July07microNuclei/NfishDataSet/OtherNFISH/20240703_144854_601_IQL_POP_033_ALTcontrol_NFISH_B1-B3/nfish_df_b3/'\n",
    "\n",
    "# List all CSV files in the directory\n",
    "csv_files = [f for f in os.listdir(directory_path) if f.endswith('.csv')]\n",
    "\n",
    "# Initialize an empty list to hold DataFrames\n",
    "dataframes = []\n",
    "\n",
    "# Loop through the list of CSV files and read them into DataFrames\n",
    "for csv_file in csv_files:\n",
    "    file_path = os.path.join(directory_path, csv_file)\n",
    "    df = pd.read_csv(file_path)\n",
    "    dataframes.append(df)\n",
    "\n",
    "# Concatenate all DataFrames into a single large DataFrame\n",
    "big_dataframe = pd.concat(dataframes, ignore_index=True)\n",
    "\n",
    "# Optionally, inspect the resulting DataFrame\n",
    "print(big_dataframe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81cac794-98e5-4c22-bfe5-791b1acb3119",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b94deb5-f72f-46fc-ab4a-adf2fc5b05c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_filtered = big_dataframe[pd.notna(big_dataframe['cell_barcode_0'])]\n",
    "len(df_filtered)\n",
    "\n",
    "df_filtered1 = df_filtered[['cell_barcode_0','IntensitySum', 'NumberOfSpots', 'AverageSpotIntensity']]\n",
    "\n",
    "df_filtered1 = df_filtered1.copy()\n",
    "df_filtered1['GeneName'] = df_filtered1['cell_barcode_0'].map(grnaMap1_short)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbfdaf9f-fe01-45dd-9692-e842d072d2c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df_filtered1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "915291f4-4193-424c-9737-48e5af87fee9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_position_from_nd2(file_path):\n",
    "    \n",
    "\n",
    "    nd2Read = nd2.ND2File(file_path)\n",
    "    \n",
    "    nd22 = nd2Read.frame_metadata(0)\n",
    "    positionsFg = nd22.channels[0]\n",
    "    xPos = positionsFg.position.stagePositionUm.x\n",
    "    yPos = positionsFg.position.stagePositionUm.y\n",
    "    nd2Read.close()\n",
    "\n",
    "    return xPos, yPos \n",
    "\n",
    "def readPhenotypeImageToExtractLocation(file_path_toRead):\n",
    "    import nd2\n",
    "    fileRead = nd2.ND2File(file_path_toRead)\n",
    "    \n",
    "    fg = fileRead.frame_metadata(0)\n",
    "    positionsFg = fg.channels[0]\n",
    "    micronPpixel = positionsFg.volume.axesCalibration[0]\n",
    "    xPos = positionsFg.position.stagePositionUm.x\n",
    "    yPos = positionsFg.position.stagePositionUm.y\n",
    "    nd2ReadFileIm = nd2.imread(file_path_toRead)\n",
    "    #print(phenotype10x.shape)\n",
    "    \n",
    "    phenotypeIm = np.zeros([2,2304,2304])\n",
    "    phenotypeIm[0] = phenotypeIm[1] = nd2ReadFileIm #nd2ReadFile[0]\n",
    "    \n",
    "    fileRead.close()\n",
    "    \n",
    "    #matplotCreationWhole(phenotype10x[1])\n",
    "    return phenotypeIm, micronPpixel, xPos, yPos\n",
    "\n",
    "\n",
    "def readPhenotypeImageToExtractLocation2(file_path_toRead):\n",
    "    import nd2\n",
    "    fileRead = nd2.ND2File(file_path_toRead)\n",
    "    \n",
    "    fg = fileRead.frame_metadata(0)\n",
    "    positionsFg = fg.channels[0]\n",
    "    micronPpixel = positionsFg.volume.axesCalibration[0]\n",
    "    xPos = positionsFg.position.stagePositionUm.x\n",
    "    yPos = positionsFg.position.stagePositionUm.y\n",
    "    nd2ReadFileIm = nd2.imread(file_path_toRead)\n",
    "    #print(phenotype10x.shape)\n",
    "    phenotypeIm = np.zeros([2,2304,2304])\n",
    "\n",
    "    #print(nd2ReadFileIm.shape)\n",
    "    if nd2ReadFileIm.ndim==2:\n",
    "        phenotypeIm[0] = phenotypeIm[1] = nd2ReadFileIm #nd2ReadFile[0]\n",
    "    \n",
    "    elif nd2ReadFileIm.ndim == 3:\n",
    "        phenotypeIm[0] = phenotypeIm[1] = nd2ReadFileIm[0] #nd2ReadFile[0]\n",
    "        #raise ValueError(\"Unexpected image dimensions: {}\".format(nd2ReadFileIm.shape))\n",
    "\n",
    "    fileRead.close()\n",
    "    \n",
    "    #matplotCreationWhole(phenotype10x[1])\n",
    "    return phenotypeIm, micronPpixel, xPos, yPos\n",
    "\n",
    "\n",
    "\n",
    "def read_nd2_getXY(file_path_toRead):\n",
    "    import nd2\n",
    "    fileRead = nd2.ND2File(file_path_toRead)\n",
    "    \n",
    "    fg = fileRead.frame_metadata(0)\n",
    "    positionsFg = fg.channels[0]\n",
    "    micronPpixel = positionsFg.volume.axesCalibration[0]\n",
    "    xPos = positionsFg.position.stagePositionUm.x\n",
    "    yPos = positionsFg.position.stagePositionUm.y\n",
    "\n",
    "    fileRead.close()\n",
    "    \n",
    "    #matplotCreationWhole(phenotype10x[1])\n",
    "    return micronPpixel, xPos, yPos\n",
    "\n",
    "# import cellpose\n",
    "\n",
    "\n",
    "def calculate_FOV_coord(x_fov, y_fov, height):\n",
    "    multiplier =0.5\n",
    "    x1_ = x_fov - multiplier*height \n",
    "    x2_ = x_fov + multiplier*height \n",
    "    \n",
    "    y1_ = y_fov - multiplier*height\n",
    "    y2_ = y_fov + multiplier*height\n",
    "    return x1_, x2_, y1_, y2_\n",
    "\n",
    "def extract_features(phenotype_, nuclei_label):\n",
    "    df_phenotype = Snake._extract_named_features(\n",
    "        data = phenotype_,\n",
    "        labels=nuclei_label,\n",
    "        feature_names=[\n",
    "            'label', # required to join SBS and phenotype data\n",
    "            'i',\n",
    "            'j',\n",
    "            'eccentricity',\n",
    "            'area',\n",
    "            'solidity',\n",
    "            'dapi_mean',\n",
    "            'perimeter'\n",
    "        ],\n",
    "    \n",
    "        wildcards=WILDCARDS\n",
    "    )\n",
    "    return df_phenotype\n",
    "\n",
    "\n",
    "\n",
    "def find_closest(row, tree, distance_threshold=20, k=100):\n",
    "    # Perform the query with a large k value to capture a sufficient number of points\n",
    "    distances, indices = tree.query([[row['x_nucleus_um_global_adjusted'], row['y_nucleus_um_global_adjusted']]], k=k)\n",
    "    \n",
    "    # Filter the results based on the distance threshold\n",
    "    filtered_results = [(dist, idx) for dist, idx in zip(distances[0], indices[0]) if dist <= distance_threshold]\n",
    "    \n",
    "    # Sort the results by distance\n",
    "    filtered_results.sort(key=lambda x: x[0])\n",
    "    \n",
    "    # Extract the indices of the 5 closest points within the distance threshold\n",
    "    closest_indices = [idx for _, idx in filtered_results[:5]]\n",
    "    \n",
    "    return closest_indices\n",
    "\n",
    "from cellpose import models\n",
    "import torch\n",
    "\n",
    "def cellposeSegNucG(img, min_size=15):\n",
    "    # Check if GPU is available and set use_gpu accordingly\n",
    "    use_gpu = torch.cuda.is_available()\n",
    "\n",
    "    model = models.Cellpose(gpu=use_gpu, model_type=\"nuclei\")\n",
    "    \n",
    "    # Include the use_gpu parameter in the eval function\n",
    "    res, _, _, _ = model.eval(\n",
    "        img,\n",
    "        channels=[0, 0],\n",
    "        diameter=None,\n",
    "        min_size=min_size,\n",
    "        #gpu=use_gpu  # Add this line\n",
    "    )\n",
    "    return res\n",
    "    \n",
    "def cellposeSegNucGInten(img, min_size=200, intensity_threshold=None):\n",
    "    # Check if GPU is available and set use_gpu accordingly\n",
    "    use_gpu = torch.cuda.is_available()\n",
    "\n",
    "    # Optionally apply an intensity threshold\n",
    "    if intensity_threshold is not None:\n",
    "        img = np.where(img > intensity_threshold, img, 0)\n",
    "\n",
    "    model = models.Cellpose(gpu=use_gpu, model_type=\"nuclei\")\n",
    "    \n",
    "    # Include the use_gpu parameter in the eval function\n",
    "    res, _, _, _ = model.eval(\n",
    "        img,\n",
    "        channels=[0, 0],\n",
    "        diameter=80,\n",
    "        min_size=min_size\n",
    "    )\n",
    "    return res\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "multiplier =0.5\n",
    "def get_20x_fov_df(input_file_k20, micronPpixel10x2, xoffset, yoffset): ## offset needs to be applied here\n",
    "    \n",
    "    phenotype20x2, micronPpixel20x2, xPos_20x2, yPos_20x2 = readPhenotypeImageToExtractLocation2(input_file_k20)\n",
    "    NucleiCellpose20x = get_segmentation(input_file_k20)\n",
    "    #NucleiCellpose20x = cellposeSegNucG(input_file_k20)\n",
    "    #NucleiCellpose20x = cellposeSegNucG(phenotype20x2[0], 1000)\n",
    "    \n",
    "    df_phenotype_20x2 = extract_features(phenotype20x2, NucleiCellpose20x)\n",
    "    height20x = 2304*micronPpixel20x2\n",
    "    \n",
    "    df_phenotype_20x2['x_nucleus_um'] =  height20x - df_phenotype_20x2['j']*micronPpixel20x2\n",
    "    df_phenotype_20x2['y_nucleus_um'] =  df_phenotype_20x2['i']*micronPpixel20x2\n",
    "    \n",
    "    df_phenotype_20x2['x_nucleus_um_global'] = xPos_20x2 - multiplier*height20x +  df_phenotype_20x2['x_nucleus_um'] \n",
    "    df_phenotype_20x2['y_nucleus_um_global'] = yPos_20x2 - multiplier*height20x +  df_phenotype_20x2['y_nucleus_um'] \n",
    "    \n",
    "    df2_t = df_phenotype_20x2.copy() # only do it first time\n",
    "    \n",
    "    uPerP10x = micronPpixel10x2\n",
    "    uPerP20x = micronPpixel20x2\n",
    "\n",
    "\n",
    "    #-25.8 136.8\n",
    "    df2_t['x_nucleus_um_global_adjusted'] =df2_t['x_nucleus_um_global'] + xoffset\n",
    "    df2_t['y_nucleus_um_global_adjusted'] =df2_t['y_nucleus_um_global'] + yoffset\n",
    "    \n",
    "    area_ratio = uPerP10x/uPerP20x\n",
    "    df2_t['area_new'] = df2_t['area']/ (area_ratio*area_ratio)\n",
    "    df2_t['perimeter_new'] = df2_t['perimeter']/ (area_ratio)\n",
    "    \n",
    "    df2_t['matchFoundindf1'] = -1\n",
    "    df2_t['which_df1_itbelongs'] = -1\n",
    "    return df2_t, uPerP20x\n",
    "\n",
    "\n",
    "\n",
    "def get_matching_10x_fovs_backup_delete(input20file, height20x, df_loc_highMag):\n",
    "    phenotype20x2, micronPpixel20x2, xPos_20x2, yPos_20x2 = readPhenotypeImageToExtractLocation2(input20file)\n",
    "    \n",
    "    \n",
    "    x1_20, x2_20, y1_20, y2_20 = calculate_FOV_coord(xPos_20x2, yPos_20x2, height20x)\n",
    "    \n",
    "    overlap_results = []\n",
    "    \n",
    "    area_20 = (x2_20 - x1_20) * (y2_20 - y1_20)\n",
    "    \n",
    "    for index10, row_10 in df_loc_highMag.iterrows():\n",
    "        x1_10, x2_10, y1_10, y2_10 = row_10['x1'], row_10['x2'], row_10['y1'], row_10['y2']\n",
    "    \n",
    "        # Calculate the overlap coordinates\n",
    "        overlap_x1 = max(x1_20, x1_10)\n",
    "        overlap_x2 = min(x2_20, x2_10)\n",
    "        overlap_y1 = max(y1_20, y1_10)\n",
    "        overlap_y2 = min(y2_20, y2_10)\n",
    "    \n",
    "        # Check if there is an overlap\n",
    "        if overlap_x1 < overlap_x2 and overlap_y1 < overlap_y2:\n",
    "            # Calculate overlap area\n",
    "            overlap_area = (overlap_x2 - overlap_x1) * (overlap_y2 - overlap_y1)\n",
    "            # Calculate overlap percentage with respect to the 20x view area\n",
    "            overlap_percentage = (overlap_area / area_20) * 100\n",
    "            overlap_results.append({\n",
    "                '10x_filename': row_10['filename'],\n",
    "                'OverlapPercentage': overlap_percentage,\n",
    "                '10x_index': index10  # Include the index of df_loc_highMag\n",
    "            })\n",
    "    \n",
    "    \n",
    "    df_t = pd.DataFrame(overlap_results)\n",
    "    return df_t\n",
    "\n",
    "\n",
    "def get_matching_10x_fovs(input20file, height20x, df_loc_highMag):\n",
    "    phenotype20x2, micronPpixel20x2, xPos_20x2, yPos_20x2 = readPhenotypeImageToExtractLocation2(input20file)\n",
    "    \n",
    "    \n",
    "    x1_20, x2_20, y1_20, y2_20 = calculate_FOV_coord(xPos_20x2, yPos_20x2, height20x)\n",
    "    \n",
    "    overlap_results = []\n",
    "    \n",
    "    area_20 = (x2_20 - x1_20) * (y2_20 - y1_20)\n",
    "    \n",
    "    for index10, row_10 in df_loc_highMag.iterrows():\n",
    "        x1_10, x2_10, y1_10, y2_10 = row_10['x1'], row_10['x2'], row_10['y1'], row_10['y2']\n",
    "    \n",
    "        # Calculate the overlap coordinates\n",
    "        overlap_x1 = max(x1_20, x1_10)\n",
    "        overlap_x2 = min(x2_20, x2_10)\n",
    "        overlap_y1 = max(y1_20, y1_10)\n",
    "        overlap_y2 = min(y2_20, y2_10)\n",
    "    \n",
    "        # Check if there is an overlap\n",
    "        if overlap_x1 < overlap_x2 and overlap_y1 < overlap_y2:\n",
    "            # Calculate overlap area\n",
    "            overlap_area = (overlap_x2 - overlap_x1) * (overlap_y2 - overlap_y1)\n",
    "            # Calculate overlap percentage with respect to the 20x view area\n",
    "            overlap_percentage = (overlap_area / area_20) * 100\n",
    "            overlap_results.append({\n",
    "                '10x_filename': row_10['filename'],\n",
    "                'OverlapPercentage': overlap_percentage,\n",
    "                '10x_index': index10  # Include the index of df_loc_highMag\n",
    "            })\n",
    "    \n",
    "    if not overlap_results:\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    \n",
    "    df_t = pd.DataFrame(overlap_results)\n",
    "    return df_t\n",
    "# function, extract 10x info\n",
    "# function, extract 10x info\n",
    "from scipy.spatial import cKDTree\n",
    "\n",
    "## make this callable from a function\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36b015d8-4c8d-4c58-91fb-97b8da5ea523",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial import cKDTree\n",
    "\n",
    "def find_match_in10x_fov_df(df_matchingrows_10x_index, input_files10, df2_, area_ratio):\n",
    "\n",
    "    for k10 in df_matchingrows_10x_index:\n",
    "        \n",
    "        ## this part should go over matching 10x fovs\n",
    "        phenotype10x2, micronPpixel10x2, xPos_10x2, yPos_10x2 = readPhenotypeImageToExtractLocation2(input_files10[k10])\n",
    "\n",
    "        if DEBUG_PRINT:\n",
    "            print(input_files10[k10])\n",
    "\n",
    "        uPerP10x = micronPpixel10x2\n",
    "        \n",
    "        #plt.imshow(phenotype10x2[0])\n",
    "        \n",
    "        \n",
    "        NucleiCellpose10x = get_segmentation(input_files10[k10])\n",
    "        #NucleiCellpose10x = cellposeSegNucG(input_files10[k10])\n",
    "        #NucleiCellpose10x = cellposeSegNucG(phenotype10x2[0], 1000)\n",
    "        \n",
    "        df_phenotype_10x2 = extract_features(phenotype10x2, NucleiCellpose10x)\n",
    "        df_phenotype_10x2['x_nucleus_um'] =  height10x - df_phenotype_10x2['j']*micronPpixel10x2\n",
    "        df_phenotype_10x2['y_nucleus_um'] =  df_phenotype_10x2['i']*micronPpixel10x2\n",
    "        \n",
    "        \n",
    "        df_phenotype_10x2['x_nucleus_um_global'] = xPos_10x2 - multiplier*height10x + df_phenotype_10x2['x_nucleus_um'] \n",
    "        df_phenotype_10x2['y_nucleus_um_global'] = yPos_10x2 - multiplier*height10x + df_phenotype_10x2['y_nucleus_um'] \n",
    "        \n",
    "        \n",
    "        \n",
    "        df1 = df_phenotype_10x2.copy()\n",
    "        df2 = df2_.copy()\n",
    "        \n",
    "        df1['x_nucleus_um_global_adjusted'] = df1['x_nucleus_um_global']\n",
    "        df1['y_nucleus_um_global_adjusted'] = df1['y_nucleus_um_global']\n",
    "\n",
    "        #area_ratio = uPerP10x/uPerP20x\n",
    "        df2['area_new'] = df2['area']/ (area_ratio*area_ratio)\n",
    "        df2['perimeter_new'] = df2['perimeter']/ (area_ratio)\n",
    "        \n",
    "        points2 = df1[['x_nucleus_um_global_adjusted', 'y_nucleus_um_global_adjusted']].to_numpy()\n",
    "        #points = df1[['area', 'dapi_mean']].to_numpy()\n",
    "        \n",
    "        # Create the KD-tree\n",
    "        tree = cKDTree(points2)\n",
    "        \n",
    "        \n",
    "        # Apply the function to each row in df2\n",
    "        df2['ClosestInDf1'] = df2.apply(lambda row: find_closest(row, tree), axis=1)\n",
    "        \n",
    "        \n",
    "        for k20 in range(len(df2)): ## go over all cell in a single FOV of a single 20x image\n",
    "        \n",
    "            matched_fovs = df2.iloc[k20]['ClosestInDf1']\n",
    "            #print(matched_fovs)\n",
    "            \n",
    "            matchFound = False\n",
    "            \n",
    "            if len(matched_fovs) > 0:\n",
    "                matchFound = False\n",
    "                matchIndex = 0\n",
    "                for k_10_matched_index in range(len(matched_fovs)):\n",
    "                    if compare_two_cells(df2.iloc[k20], df1.iloc[df2.iloc[k20]['ClosestInDf1'][k_10_matched_index]],0.1):\n",
    "                        matchFound = True\n",
    "                        df2.loc[k20, 'matchFoundindf1'] = matched_fovs[k_10_matched_index]\n",
    "                        df2.loc[k20, 'which_df1_itbelongs'] = k10\n",
    "                        #print(k20)\n",
    "                        break\n",
    "\n",
    "    return df2, df1\n",
    "\n",
    "\n",
    "\n",
    "import os\n",
    "from skimage import io\n",
    "\n",
    "def get_segmentation(imagePath):\n",
    "    # Extract the filename without the extension\n",
    "    filename = os.path.basename(imagePath)\n",
    "    \n",
    "    # Determine the path to the \"Segmented\" folder\n",
    "    path_only = os.path.dirname(imagePath)\n",
    "    segmented_folder = os.path.join(path_only, 'Segmented')\n",
    "    \n",
    "    # Construct the path to the segmented image with the .png extension\n",
    "    segmentedImageName = os.path.join(segmented_folder, os.path.splitext(filename)[0] + '_segmented.png')\n",
    "    \n",
    "    # Read the segmented image\n",
    "    segmented_DAPI = io.imread(segmentedImageName)\n",
    "    \n",
    "    return segmented_DAPI\n",
    "\n",
    "\n",
    "def get_nfish_preanalyzed(imagePath, nfishPath):\n",
    "    # Extract the filename without the extension\n",
    "    filename = os.path.basename(imagePath)\n",
    "    \n",
    "    # Determine the path to the \"Segmented\" folder\n",
    "    path_only = os.path.dirname(imagePath)\n",
    "    segmented_folder = os.path.join(path_only, nfishPath)\n",
    "    \n",
    "    # Construct the path to the segmented image with the .png extension\n",
    "    df_cells_nf = pd.read_csv(os.path.join(segmented_folder, os.path.splitext(filename)[0] + '_cells.csv'))\n",
    "    df_spots_nf = pd.read_csv(os.path.join(segmented_folder, os.path.splitext(filename)[0] + '_spots.csv'))\n",
    "        \n",
    "    return df_cells_nf, df_spots_nf\n",
    "\n",
    "\n",
    "\n",
    "## matching using morphological parameters\n",
    "import math\n",
    "\n",
    "def compare_two_cells(c20, c10, thresh_morph):\n",
    "    perimeterCheck = check_within_range(c20['perimeter_new'], c10['perimeter'], thresh_morph)\n",
    "    \n",
    "    areaCheck = check_within_range(c20['area_new'], c10['area'], thresh_morph)\n",
    "    eccenCheck = check_within_range(c20['eccentricity'], c10['eccentricity'], thresh_morph)\n",
    "    solidityCheck = check_within_range(c20['solidity'], c10['solidity'], thresh_morph)\n",
    "\n",
    "    totalCheck = perimeterCheck*1 + areaCheck*1 + eccenCheck*1 + solidityCheck*1\n",
    "    #print(perimeterCheck, areaCheck,eccenCheck, solidityCheck )\n",
    "    #return perimeterCheck&areaCheck&eccenCheck&solidityCheck\n",
    "    return totalCheck>2\n",
    "    \n",
    "\n",
    "def check_within_range(v20,v10, thresh_morph = 0.15):\n",
    "    change_val = abs(v20-v10)/v20\n",
    "    return change_val < thresh_morph\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def cell_match_finder(i10, df2_, df1_, thresh_morph):\n",
    "    cell_in_20x = df2_.iloc[i10] # get cell form 20x FOV\n",
    "    no_of_cells = len(cell_in_20x['ClosestInDf1']) # number of match found using distance\n",
    "\n",
    " \n",
    "    found = False \n",
    "    count = -1\n",
    "    for i in range(no_of_cells):\n",
    "        cellID_closest = cell_in_20x['ClosestInDf1'][i]\n",
    "        if DEBUG_PRINT:\n",
    "            print(cellID_closest)\n",
    "        cell_Closest_in_10x = df1_.iloc[cellID_closest]\n",
    "        count = i\n",
    "        found = compare_two_cells(cell_in_20x, cell_Closest_in_10x, thresh_morph)\n",
    "        if(found):\n",
    "            #print(f\"match found {i} location\")\n",
    "            break\n",
    "\n",
    "    \n",
    "\n",
    "    return found, cell_in_20x['ClosestInDf1'][count] \n",
    "\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3844a90e-e221-43a4-b2bf-aa6f13ef02e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_matched_df_from_phenotype(input_20x, directory_df_10x_iter, df_loc10_local, nfishPath, xoffset, yoffset ):\n",
    "    \n",
    "    #df2_b, uPerP20x = get_20x_fov_df(input_20x, micronPpixel10x2, xoffset = -25.8, yoffset = 136.8) # with offset adjustment gets the dataframe for the 20x FOC\n",
    "    df2_b_f, uPerP20x = get_20x_fov_df(input_20x, micronPpixel10x2, xoffset, yoffset) # with offset adjustment gets the dataframe for the 20x FOC\n",
    "    \n",
    "    height10x = 2304*micronPpixel10x2\n",
    "    height20x = 2304*uPerP20x\n",
    "    \n",
    "    # updates the dataframe df2 with matching coordinate from 10x FOV. This should be updated to load saved FOV,  now it calculates the df1 from extracting phenotype\n",
    "    \n",
    "    area_ratio = micronPpixel10x2/uPerP20x\n",
    "    df_matchingrows = get_matching_10x_fovs(input_20x, height20x, df_loc10_local)\n",
    "\n",
    "    if DEBUG_PRINT:\n",
    "        print(df_matchingrows)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    if df_matchingrows.empty:\n",
    "        if DEBUG_PRINT:\n",
    "            print(\"The DataFrame df_matchingrows is empty.\")\n",
    "        return pd.DataFrame()\n",
    "    else:\n",
    "        if DEBUG_PRINT:\n",
    "            print(df_matchingrows)\n",
    "    \n",
    "        #df2_calc_f, df1 = find_match_in10x_fov_df(df_matchingrows['10x_index'], input_files10, df2_b_f, area_ratio)\n",
    "\n",
    "        df2_calc_f = find_match_in10x_fov_df_modified(df_matchingrows['10x_index'], input_files10, df2_b_f, area_ratio)\n",
    "        if DEBUG_PRINT:\n",
    "            print(directory_df_10x_iter)\n",
    "        df_cells_read_10x_iter = get_df_cells_matching(df_matchingrows.iloc[0]['10x_index'], directory_df_10x_iter)\n",
    "        \n",
    "        df2_calc_f['real_cell_label'] = df2_calc_f['matchFoundindf1'].apply(lambda x: x + 1 if x > 0 else x)\n",
    "                \n",
    "        df_merged_f = pd.merge(df2_calc_f, df_cells_read_10x_iter, left_on='real_cell_label', right_on='cell', how='left')\n",
    "    \n",
    "        df_cell_nf, df_spots_nf = get_nfish_preanalyzed(input_20x, nfishPath)\n",
    "        \n",
    "        df_merged_with_nf = pd.merge(df_merged_f, df_cell_nf, left_on='label', right_on='CellLabel', how='left')\n",
    "        return df_merged_with_nf\n",
    "        \n",
    "\n",
    "\n",
    "def get_df_cells_matching(positionInSeries_df_cells, directory_df_10x_iter_df_get):\n",
    "\n",
    "    #positionInSeries = df_matchingrows.iloc[0]['10x_index']\n",
    "    \n",
    "    locationInFolder = 0\n",
    "    #positionInSeries = df_matchingrows.iloc[0]['10x_index']\n",
    "    df_cells_read_10x_iter = pd.read_csv(directory_df_10x_iter_df_get + f\"df_cells_Well{locationInFolder }_Position{positionInSeries_df_cells:02}.csv\")\n",
    "\n",
    "    return df_cells_read_10x_iter\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def save_matched_df(input_20x, df_cells_nf, nfish_df_directory):\n",
    "\n",
    "    filename = os.path.basename(input_20x)\n",
    "    path_only = os.path.dirname(input_20x)\n",
    "    segmented_folder = os.path.join(path_only, nfish_df_directory)\n",
    "    # Construct the path to the segmented image with the .png extension\n",
    "    save_csv = os.path.join(segmented_folder, os.path.splitext(filename)[0] + '_cells.csv')\n",
    "    if DEBUG_PRINT:\n",
    "        print('file_written', save_csv)\n",
    "    df_cells_nf.to_csv(save_csv)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac43585a-ecb4-4825-ad37-79c7ebecab74",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from scipy.spatial import cKDTree\n",
    "\n",
    "def find_match_in10x_fov_df_modified(df_matchingrows_10x_index, input_files10, df2_, area_ratio):\n",
    "    df2 = df2_.copy()  # Make a copy of df2 to modify\n",
    "\n",
    "    # Loop through matching 10x FOVs\n",
    "    for k10 in df_matchingrows_10x_index:\n",
    "        # Work only on rows where 'which_df1_itbelongs' == -1\n",
    "        df2_subset = df2.loc[df2['which_df1_itbelongs'] == -1].copy()\n",
    "\n",
    "        # Adjust `area_new` and `perimeter_new` for df2_subset\n",
    "        df2_subset.loc[:, 'area_new'] = df2_subset['area'] / (area_ratio * area_ratio)\n",
    "        df2_subset.loc[:, 'perimeter_new'] = df2_subset['perimeter'] / area_ratio\n",
    "\n",
    "        # Process each matching 10x FOV\n",
    "        phenotype10x2, micronPpixel10x2, xPos_10x2, yPos_10x2 = readPhenotypeImageToExtractLocation2(input_files10[k10])\n",
    "        print(f'Processing file: {input_files10[k10]}')\n",
    "\n",
    "        # Calculate scale and coordinates\n",
    "        uPerP10x = micronPpixel10x2\n",
    "        NucleiCellpose10x = get_segmentation(input_files10[k10])\n",
    "\n",
    "        # Extract features from the 10x image\n",
    "        df_phenotype_10x2 = extract_features(phenotype10x2, NucleiCellpose10x)\n",
    "        df_phenotype_10x2['x_nucleus_um'] = height10x - df_phenotype_10x2['j'] * micronPpixel10x2\n",
    "        df_phenotype_10x2['y_nucleus_um'] = df_phenotype_10x2['i'] * micronPpixel10x2\n",
    "\n",
    "        # Global coordinates for the 10x image nuclei\n",
    "        df_phenotype_10x2['x_nucleus_um_global'] = xPos_10x2 - multiplier * height10x + df_phenotype_10x2['x_nucleus_um']\n",
    "        df_phenotype_10x2['y_nucleus_um_global'] = yPos_10x2 - multiplier * height10x + df_phenotype_10x2['y_nucleus_um']\n",
    "\n",
    "        # Adjusted global coordinates\n",
    "        df_phenotype_10x2['x_nucleus_um_global_adjusted'] = df_phenotype_10x2['x_nucleus_um_global']\n",
    "        df_phenotype_10x2['y_nucleus_um_global_adjusted'] = df_phenotype_10x2['y_nucleus_um_global']\n",
    "\n",
    "        # Create a KDTree from the 10x nuclei coordinates\n",
    "        points2 = df_phenotype_10x2[['x_nucleus_um_global_adjusted', 'y_nucleus_um_global_adjusted']].to_numpy()\n",
    "        tree = cKDTree(points2)\n",
    "\n",
    "        # Apply the find_closest function to each row in df2_subset\n",
    "        df2_subset['ClosestInDf1'] = df2_subset.apply(lambda row: find_closest(row, tree), axis=1)\n",
    "\n",
    "        # Transfer `ClosestInDf1` to the original `df2`\n",
    "        df2.loc[df2_subset.index, 'ClosestInDf1'] = df2_subset['ClosestInDf1']\n",
    "\n",
    "        # Loop over each row in df2_subset to find matches\n",
    "        for k20 in range(len(df2_subset)):\n",
    "            matched_fovs = df2_subset.iloc[k20]['ClosestInDf1']\n",
    "\n",
    "            if len(matched_fovs) > 0:\n",
    "                for k_10_matched_index in range(len(matched_fovs)):\n",
    "                    if compare_two_cells(df2_subset.iloc[k20], df_phenotype_10x2.iloc[matched_fovs[k_10_matched_index]], 0.1):\n",
    "                        # Only update if it has not been matched before or a new match is found\n",
    "                        if df2_subset.iloc[k20]['matchFoundindf1'] == -1:\n",
    "                            df2.loc[df2_subset.index[k20], 'matchFoundindf1'] = matched_fovs[k_10_matched_index]\n",
    "                            df2.loc[df2_subset.index[k20], 'which_df1_itbelongs'] = k10\n",
    "\n",
    "                        # Update the adjusted values for `area_new` and `perimeter_new`\n",
    "                        df2.loc[df2_subset.index[k20], 'area_new'] = df2.loc[df2_subset.index[k20], 'area'] / (area_ratio * area_ratio)\n",
    "                        df2.loc[df2_subset.index[k20], 'perimeter_new'] = df2.loc[df2_subset.index[k20], 'perimeter'] / area_ratio\n",
    "                        break\n",
    "\n",
    "        if DEBUG_PRINT:\n",
    "            print(f'Matches found in this iteration: {len(df2[df2[\"matchFoundindf1\"] != -1])}')\n",
    "\n",
    "    return df2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6050235-96a5-48cd-a9f0-7734dc839b18",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a075c0a3-5b61-403b-b83b-d37dc67dc55d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03efa991-ae28-42a4-b430-550a69ccdc2d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df581fa7-2bef-4a3c-9d85-6553f91cb1cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ca52693-d073-4820-8488-e7fa889402cb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "2022Wrk",
   "language": "python",
   "name": "2022wrk"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
